{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise12-TreeReduce.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtBFh9QwOlt",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 12 - Tree Reduce\n",
        "\n",
        "**GOAL:** The goal of this exercise is to show how to implement a tree reduce in Ray by passing object IDs into remote functions to encode dependencies between tasks.\n",
        "\n",
        "In this exercise, you will use Ray to implement parallel data generation and a parallel tree reduction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WjH9_hawXM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5b02632-a57e-4cc8-bc67-620a03ca81b4"
      },
      "source": [
        "import os\n",
        "os.system('pip install ray')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QL_xWobwOly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import ray\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Nj8Df9wOmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5b19aeb2-bccd-4f2b-b1cb-bab991e4471a"
      },
      "source": [
        "ray.init(num_cpus=8, include_webui=False, ignore_reinit_error=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 13:34:58,370\tWARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
            "2019-05-16 13:34:58,372\tINFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_13-34-58_723/logs.\n",
            "2019-05-16 13:34:58,490\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:17401 to respond...\n",
            "2019-05-16 13:34:58,622\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:18519 to respond...\n",
            "2019-05-16 13:34:58,628\tINFO services.py:804 -- Starting Redis shard with 2.58 GB max memory.\n",
            "2019-05-16 13:34:58,665\tINFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_13-34-58_723/logs.\n",
            "2019-05-16 13:34:58,667\tINFO services.py:1427 -- Starting the Plasma object store with 3.87 GB memory using /dev/shm.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2019-05-16_13-34-58_723/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2019-05-16_13-34-58_723/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:17401',\n",
              " 'webui_url': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_Uh4dvwOmM",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** These functions will need to be turned into remote functions so that the tree of tasks can be executed in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI9zaUxOwOmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a proxy for a function which generates some data.\n",
        "@ray.remote\n",
        "def create_data(i):\n",
        "    time.sleep(0.3)\n",
        "    return i * np.ones(10000)\n",
        "\n",
        "# This is a proxy for an expensive aggregation step (which is also\n",
        "# commutative and associative so it can be used in a tree-reduce).\n",
        "@ray.remote\n",
        "def aggregate_data(x, y):\n",
        "    time.sleep(0.3)\n",
        "    return x * y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6O7Fjw2wOmZ",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** Make the data creation tasks run in parallel. Also aggregate the vectors in parallel. Note that the `aggregate_data` function must be called 7 times. They cannot all run in parallel because some depend on the outputs of others. However, it is possible to first run 4 in parallel, then 2 in parallel, and then 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBmc13JqwOmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sleep a little to improve the accuracy of the timing measurements below.\n",
        "time.sleep(1.0)\n",
        "start_time = time.time()\n",
        "\n",
        "# EXERCISE: Here we generate some data. Do this part in parallel.\n",
        "vectors = [create_data.remote(i + 1) for i in range(8)]\n",
        "\n",
        "# Here we aggregate all of the data repeatedly calling aggregate_data. This\n",
        "# can be sped up using Ray.\n",
        "#\n",
        "# NOTE: A direct translation of the code below to use Ray will not result in\n",
        "# a speedup because each function call uses the output of the previous function\n",
        "# call so the function calls must be executed serially.\n",
        "#\n",
        "# EXERCISE: Speed up the aggregation below by using Ray. Note that this will\n",
        "# require restructuring the code to expose more parallelism. First run 4 tasks\n",
        "# aggregating the 8 values in pairs. Then run 2 tasks aggregating the resulting\n",
        "# 4 intermediate values in pairs. then run 1 task aggregating the two resulting\n",
        "# values. Lastly, you will need to call ray.get to retrieve the final result.\n",
        "#\n",
        "# Exposing more parallelism means aggregating the vectors in a DIFFERENT ORDER.\n",
        "# This can be done because we are simply summing the data and the order in\n",
        "# which the values are summed doesn't matter (it's commutative and associative).\n",
        "\n",
        "\n",
        "stride = 1;\n",
        "length = len(vectors)\n",
        "\n",
        "while stride < len(vectors):\n",
        "  for i in range(0,length, stride*2):\n",
        "    vectors[i] = aggregate_data.remote(vectors[i], vectors[i+stride])\n",
        "  stride = stride*2\n",
        "\n",
        "# NOTE: For clarity, the aggregation above is written out as 7 separate function\n",
        "# calls, but this can be done more easily in a while loop via\n",
        "#\n",
        "#     while len(vectors) > 1:\n",
        "#         vectors = aggregate_data(vectors[0], vectors[1]) + vectors[2:]\n",
        "#     result = vectors[0]\n",
        "#\n",
        "# When expressed this way, the change from serial aggregation to tree-structured\n",
        "# aggregation can be made simply by appending the result of aggregate_data to the\n",
        "# end of the vectors list as opposed to the beginning.\n",
        "#\n",
        "# EXERCISE: Think about why this is true.\n",
        "result = ray.get(vectors[0])\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS824u3rwOmg",
        "colab_type": "text"
      },
      "source": [
        "**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThKbLcTlwOmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91964654-54f6-4d57-f248-455abee23273"
      },
      "source": [
        "assert np.all(result == 40320 * np.ones(10000)), ('Did you remember to '\n",
        "                                                  'call ray.get?')\n",
        "assert duration < 0.3 + 0.9 + 0.3, ('FAILURE: The data generation and '\n",
        "                                    'aggregation took {} seconds. This is '\n",
        "                                    'too slow'.format(duration))\n",
        "assert duration > 0.3 + 0.9, ('FAILURE: The data generation and '\n",
        "                              'aggregation took {} seconds. This is '\n",
        "                              'too fast'.format(duration))\n",
        "\n",
        "print('Success! The example took {} seconds.'.format(duration))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! The example took 1.2181873321533203 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wVg_gZ4wOmq",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** Use the UI to view the task timeline and to verify that the vectors were aggregated with a tree of tasks.\n",
        "\n",
        "You should be able to see the 8 `create_data` tasks running in parallel followed by 4 `aggregate_data` tasks running in parallel followed by 2 more `aggregate_data` tasks followed by 1 more `aggregate_data` task.\n",
        "\n",
        "In the timeline, click on **View Options** and select **Flow Events** to visualize tasks dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgi2jTHIwOms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ray.global_state.chrome_tracing_dump(filename=\"/tmp/timeline01.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xli4MEzZ3C_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}