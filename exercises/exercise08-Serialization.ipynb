{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise08-Serialization.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaRa5SXwlM8W",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 8 - Speed up Serialization\n",
        "\n",
        "**GOAL:** The goal of this exercise is to illustrate how to speed up serialization by using `ray.put`.\n",
        "\n",
        "### Concepts for this Exercise - ray.put\n",
        "\n",
        "Object IDs can be created in multiple ways.\n",
        "- They are returned by remote function calls.\n",
        "- They are returned by actor method calls.\n",
        "- They are returned by `ray.put`.\n",
        "\n",
        "When an object is passed to `ray.put`, the object is serialized using the Apache Arrow format (see https://arrow.apache.org/ for more information about Arrow) and copied into a shared memory object store. This object will then be available to other workers on the same machine via shared memory. If it is needed by workers on another machine, it will be shipped under the hood.\n",
        "\n",
        "**When objects are passed into a remote function, Ray puts them in the object store under the hood.** That is, if `f` is a remote function, the code\n",
        "\n",
        "```python\n",
        "x = np.zeros(1000)\n",
        "f.remote(x)\n",
        "```\n",
        "\n",
        "is essentially transformed under the hood to\n",
        "\n",
        "```python\n",
        "x = np.zeros(1000)\n",
        "x_id = ray.put(x)\n",
        "f.remote(x_id)\n",
        "```\n",
        "\n",
        "The call to `ray.put` copies the numpy array into the shared-memory object store, from where it can be read by all of the worker processes (without additional copying). However, if you do something like\n",
        "\n",
        "```python\n",
        "for i in range(10):\n",
        "    f.remote(x)\n",
        "```\n",
        "\n",
        "then 10 copies of the array will be placed into the object store. This takes up more memory in the object store than is necessary, and it also takes time to copy the array into the object store over and over. This can be made more efficient by placing the array in the object store only once as follows.\n",
        "\n",
        "```python\n",
        "x_id = ray.put(x)\n",
        "for i in range(10):\n",
        "    f.remote(x_id)\n",
        "```\n",
        "\n",
        "In this exercise, you will speed up the code below and reduce the memory footprint by calling `ray.put` on the neural net weights before passing them into the remote functions.\n",
        "\n",
        "**WARNING:** This exercise requires a lot of memory to run. If this notebook is running within a Docker container, then the docker container must be started with a large shared-memory file system. This can be done by starting the docker container with the `--shm-size` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEOgYYQIluan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a92ddcf-1d8d-4d96-94bd-d71cdf05ce94"
      },
      "source": [
        "import os\n",
        "os.system('pip install ray')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUink4vMlM8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import ray\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DvPFMEOlM8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8a245343-0f1f-485b-be46-40d637f49062"
      },
      "source": [
        "ray.init(num_cpus=4, include_webui=False, ignore_reinit_error=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 12:21:49,942\tWARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
            "2019-05-16 12:21:49,949\tINFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_12-21-49_125/logs.\n",
            "2019-05-16 12:21:50,064\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:22136 to respond...\n",
            "2019-05-16 12:21:50,194\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:35395 to respond...\n",
            "2019-05-16 12:21:50,198\tINFO services.py:804 -- Starting Redis shard with 2.58 GB max memory.\n",
            "2019-05-16 12:21:50,232\tINFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_12-21-49_125/logs.\n",
            "2019-05-16 12:21:50,235\tINFO services.py:1427 -- Starting the Plasma object store with 3.87 GB memory using /dev/shm.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2019-05-16_12-21-49_125/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2019-05-16_12-21-49_125/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:22136',\n",
              " 'webui_url': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgYv3d2ElM83",
        "colab_type": "text"
      },
      "source": [
        "Define some neural net weights which will be passed into a number of tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3TpdEf6lM87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_net_weights = {'variable{}'.format(i): np.random.normal(size=2**18)\n",
        "                      for i in range(50)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFIz_VgflM9J",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** Compare the time required to serialize the neural net weights and copy them into the object store using Ray versus the time required to pickle and unpickle the weights. The big win should be with the time required for *deserialization*.\n",
        "\n",
        "Note that when you call `ray.put`, in addition to serializing the object, we are copying it into shared memory where it can be efficiently accessed by other workers on the same machine.\n",
        "\n",
        "**NOTE:** You don't actually have to do anything here other than run the cell below and read the output.\n",
        "\n",
        "**NOTE:** Sometimes `ray.put` can be faster than `pickle.dumps`. This is because `ray.put` leverages multiple threads when serializing large objects. Note that this is not possible with `pickle`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG6b1qhClM9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "38d2a661-6e41-403a-bbee-bb0c08854e4c"
      },
      "source": [
        "print('Ray - serializing')\n",
        "%time x_id = ray.put(neural_net_weights)\n",
        "print('\\nRay - deserializing')\n",
        "%time x_val = ray.get(x_id)\n",
        "\n",
        "print('\\npickle - serializing')\n",
        "%time serialized = pickle.dumps(neural_net_weights)\n",
        "print('\\npickle - deserializing')\n",
        "%time deserialized = pickle.loads(serialized)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ray - serializing\n",
            "CPU times: user 55.3 ms, sys: 130 ms, total: 185 ms\n",
            "Wall time: 130 ms\n",
            "\n",
            "Ray - deserializing\n",
            "CPU times: user 1.13 ms, sys: 23 Âµs, total: 1.15 ms\n",
            "Wall time: 1.08 ms\n",
            "\n",
            "pickle - serializing\n",
            "CPU times: user 125 ms, sys: 175 ms, total: 300 ms\n",
            "Wall time: 300 ms\n",
            "\n",
            "pickle - deserializing\n",
            "CPU times: user 47.5 ms, sys: 8.95 ms, total: 56.4 ms\n",
            "Wall time: 56.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI_HKmMOlM9U",
        "colab_type": "text"
      },
      "source": [
        "Define a remote function which uses the neural net weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPgPiBCDlM9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@ray.remote\n",
        "def use_weights(weights, i):\n",
        "    return i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6scw5ElM9f",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** In the code below, use `ray.put` to avoid copying the neural net weights to the object store multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "botg6l7wlM9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sleep a little to improve the accuracy of the timing measurements below.\n",
        "time.sleep(2.0)\n",
        "start_time = time.time()\n",
        "nnw_id = ray.put(neural_net_weights)\n",
        "results = ray.get([use_weights.remote(nnw_id, i)\n",
        "                   for i in range(20)])\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_atRI3lM91",
        "colab_type": "text"
      },
      "source": [
        "**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTw7jz5SlM94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2280df0-4674-498d-f0de-4bb0b287de5f"
      },
      "source": [
        "assert results == list(range(20))\n",
        "assert duration < 1, ('The experiments ran in {} seconds. This is too '\n",
        "                      'slow.'.format(duration))\n",
        "\n",
        "print('Success! The example took {} seconds.'.format(duration))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! The example took 0.07344412803649902 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nDmmLtAlM-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}