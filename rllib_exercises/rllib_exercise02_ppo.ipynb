{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rllib_exercise02_ppo.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkU0fSHg9Kcx",
        "colab_type": "text"
      },
      "source": [
        "# RL Exercise 2 - Proximal Policy Optimization\n",
        "\n",
        "**GOAL:** The goal of this exercise is to demonstrate how to use the proximal policy optimization (PPO) algorithm.\n",
        "\n",
        "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
        "\n",
        "PPO is described in detail in https://arxiv.org/abs/1707.06347. It is a variant of Trust Region Policy Optimization (TRPO) described in https://arxiv.org/abs/1502.05477\n",
        "\n",
        "PPO works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
        "\n",
        "![ppo](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/ppo.png)\n",
        "\n",
        "**NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary (for this to work, you must be using a machine that has GPUs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bd1mwBF9YLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "af419f5e-ae1e-4325-9cfa-ee37e033ffd1"
      },
      "source": [
        "import os\n",
        "!pip install ray\n",
        "!pip install lz4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.6/dist-packages (0.6.6)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from ray) (1.16.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray) (0.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from ray) (1.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.10)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray) (3.6.4)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray) (1.12.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ray) (3.6.6)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.6/dist-packages (from ray) (3.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray) (7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (7.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (41.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (1.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (19.1.0)\n",
            "Collecting lz4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/fe/66da85ed881031de7cf7de9dd38cc98aec8859824c7bcd3e8a88d255f36d/lz4-2.1.6-cp36-cp36m-manylinux1_x86_64.whl (359kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 1.7MB/s \n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmjQ_scv9Kc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOAgent, DEFAULT_CONFIG\n",
        "from ray.tune.logger import pretty_print"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9xw_aKc9KdE",
        "colab_type": "text"
      },
      "source": [
        "Start up Ray. This must be done before we instantiate any RL agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb_kcf_D9KdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d426cd4f-caf9-4dcc-e909-5904c25b578c"
      },
      "source": [
        "ray.init(ignore_reinit_error=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:06:11,509\tWARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
            "2019-05-16 14:06:11,513\tINFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_14-06-11_118/logs.\n",
            "2019-05-16 14:06:11,641\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:35248 to respond...\n",
            "2019-05-16 14:06:11,780\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:64990 to respond...\n",
            "2019-05-16 14:06:11,784\tINFO services.py:804 -- Starting Redis shard with 2.58 GB max memory.\n",
            "2019-05-16 14:06:11,843\tINFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-16_14-06-11_118/logs.\n",
            "2019-05-16 14:06:11,846\tINFO services.py:1427 -- Starting the Plasma object store with 3.87 GB memory using /dev/shm.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2019-05-16_14-06-11_118/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2019-05-16_14-06-11_118/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:35248',\n",
              " 'webui_url': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO3vuuat9KdQ",
        "colab_type": "text"
      },
      "source": [
        "Instantiate a PPOAgent object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
        "\n",
        "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
        "- `num_sgd_iter` is the number of epochs of SGD (passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO.\n",
        "- `sgd_minibatch_size` is the SGD batch size that will be used to optimize the PPO surrogate objective.\n",
        "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf8oXOgR9KdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "cb429204-7053-474d-de80-8bdd9addabe8"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config['num_workers'] = 1\n",
        "config['num_sgd_iter'] = 30\n",
        "config['sgd_minibatch_size'] = 128\n",
        "config['model']['fcnet_hiddens'] = [100, 100]\n",
        "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
        "\n",
        "agent = PPOAgent(config, 'CartPole-v0')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:07:37,459\tWARNING __init__.py:21 -- DeprecationWarning: PPOAgent has been renamed to PPOTrainer. This will raise an error in the future.\n",
            "2019-05-16 14:07:37,472\tWARNING ppo.py:171 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
            "2019-05-16 14:07:37,489\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/action_dist.py:114: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:07:39,005\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7f32f54bf160>}\n",
            "2019-05-16 14:07:39,006\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f32f54b1e10>}\n",
            "2019-05-16 14:07:39,009\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f32f549dcf8>}\n",
            "2019-05-16 14:07:39,040\tINFO multi_gpu_optimizer.py:78 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaanVY9e9Kde",
        "colab_type": "text"
      },
      "source": [
        "Train the policy on the `CartPole-v0` environment for 2 steps. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0.\n",
        "\n",
        "**EXERCISE:** Inspect how well the policy is doing by looking for the lines that say something like\n",
        "\n",
        "```\n",
        "total reward is  22.3215974777\n",
        "trajectory length mean is  21.3215974777\n",
        "```\n",
        "\n",
        "This indicates how much reward the policy is receiving and how many time steps of the environment the policy ran. The maximum possible reward for this problem is 200. The reward and trajectory length are very close because the agent receives a reward of one for every time step that it survives (however, that is specific to this environment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYasTTsR9Kdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3380
        },
        "outputId": "385a87e4-2893-4fb4-b848-c990e565e244"
      },
      "source": [
        "for i in range(2):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,172\tINFO policy_evaluator.py:437 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,173\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.048, max=0.049, mean=-0.01)}}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,173\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,173\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.049, mean=-0.01)\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,174\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.049, mean=-0.01)\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,174\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'info': None,\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.048, max=0.049, mean=-0.01),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,174\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,254\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.001, max=0.001, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,269\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((13,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.769),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'advantages': np.ndarray((13,), dtype=float32, min=0.996, max=12.248, mean=6.725),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'agent_index': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'behaviour_logits': np.ndarray((13, 2), dtype=float32, min=-0.004, max=0.001, mean=-0.002),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'dones': np.ndarray((13,), dtype=bool, min=0.0, max=1.0, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'eps_id': np.ndarray((13,), dtype=int64, min=506541797.0, max=506541797.0, mean=506541797.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'infos': np.ndarray((13,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'new_obs': np.ndarray((13, 4), dtype=float32, min=-2.181, max=1.358, mean=-0.114),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'obs': np.ndarray((13, 4), dtype=float32, min=-2.077, max=1.355, mean=-0.098),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'prev_actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.692),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'prev_rewards': np.ndarray((13,), dtype=float32, min=0.0, max=1.0, mean=0.923),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'rewards': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         't': np.ndarray((13,), dtype=int64, min=0.0, max=12.0, mean=6.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'unroll_id': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'value_targets': np.ndarray((13,), dtype=float32, min=1.0, max=12.248, mean=6.728),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m                         'vf_preds': np.ndarray((13,), dtype=float32, min=-0.0, max=0.004, mean=0.002)},\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m               'type': 'SampleBatch'}}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m 2019-05-16 14:08:03,487\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.465),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.996, max=42.464, mean=12.91),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.004, max=0.005, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.045),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=93652555.0, max=1529454909.0, mean=682086737.49),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.181, max=2.189, mean=0.049),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.077, max=2.189, mean=0.044),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.435),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.95),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=54.0, mean=13.52),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.998, max=42.465, mean=12.909),\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.004, max=0.004, mean=-0.001)},\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m   'type': 'SampleBatch'}\n",
            "\u001b[2m\u001b[36m(pid=215)\u001b[0m \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:08:08,104\tINFO multi_gpu_impl.py:144 -- Training on concatenated sample batches:\n",
            "\n",
            "{ 'inputs': [ np.ndarray((4000, 4), dtype=float32, min=-2.56, max=2.555, mean=0.0),\n",
            "              np.ndarray((4000,), dtype=float32, min=0.996, max=57.441, mean=12.418),\n",
            "              np.ndarray((4000,), dtype=float32, min=-1.244, max=4.904, mean=-0.0),\n",
            "              np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.485),\n",
            "              np.ndarray((4000, 2), dtype=float32, min=-0.012, max=0.009, mean=0.0),\n",
            "              np.ndarray((4000,), dtype=float32, min=-0.008, max=0.006, mean=-0.0),\n",
            "              np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.465),\n",
            "              np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.954)],\n",
            "  'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>,\n",
            "                    <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
            "                    <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
            "                    <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
            "                    <tf.Tensor 'default_policy/logits:0' shape=(?, 2) dtype=float32>,\n",
            "                    <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
            "                    <tf.Tensor 'default_policy/action_1:0' shape=(?,) dtype=int64>,\n",
            "                    <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>],\n",
            "  'state_inputs': []}\n",
            "\n",
            "2019-05-16 14:08:08,106\tINFO multi_gpu_impl.py:189 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-08-11\n",
            "done: false\n",
            "episode_len_mean: 21.775956284153004\n",
            "episode_reward_max: 85.0\n",
            "episode_reward_mean: 21.775956284153004\n",
            "episode_reward_min: 9.0\n",
            "episodes_this_iter: 183\n",
            "episodes_total: 183\n",
            "experiment_id: fd89ac60471b404880810e4490722e65\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 3304.501\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.19999995827674866\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6646150946617126\n",
            "      kl: 0.02912997081875801\n",
            "      policy_loss: -0.039056196808815\n",
            "      total_loss: 173.78334045410156\n",
            "      vf_explained_var: 0.022880150005221367\n",
            "      vf_loss: 173.81658935546875\n",
            "  load_time_ms: 124.932\n",
            "  num_steps_sampled: 4000\n",
            "  num_steps_trained: 3968\n",
            "  sample_time_ms: 4947.051\n",
            "  update_time_ms: 827.049\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 1\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.08069184743532742\n",
            "  mean_inference_ms: 0.9122488946206746\n",
            "  mean_processing_ms: 0.2086469454337465\n",
            "time_since_restore: 9.293007612228394\n",
            "time_this_iter_s: 9.293007612228394\n",
            "time_total_s: 9.293007612228394\n",
            "timestamp: 1558015691\n",
            "timesteps_since_restore: 4000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 4000\n",
            "training_iteration: 1\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-08-17\n",
            "done: false\n",
            "episode_len_mean: 37.271028037383175\n",
            "episode_reward_max: 119.0\n",
            "episode_reward_mean: 37.271028037383175\n",
            "episode_reward_min: 11.0\n",
            "episodes_this_iter: 107\n",
            "episodes_total: 290\n",
            "experiment_id: fd89ac60471b404880810e4490722e65\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 2902.065\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.30000004172325134\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6185866594314575\n",
            "      kl: 0.01719854772090912\n",
            "      policy_loss: -0.029245762154459953\n",
            "      total_loss: 301.6075439453125\n",
            "      vf_explained_var: 0.006509021390229464\n",
            "      vf_loss: 301.631591796875\n",
            "  load_time_ms: 62.925\n",
            "  num_steps_sampled: 8000\n",
            "  num_steps_trained: 7936\n",
            "  sample_time_ms: 4326.198\n",
            "  update_time_ms: 415.91\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 1\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.07429869081687666\n",
            "  mean_inference_ms: 0.7978597740399332\n",
            "  mean_processing_ms: 0.1819815550457164\n",
            "time_since_restore: 15.51209282875061\n",
            "time_this_iter_s: 6.219085216522217\n",
            "time_total_s: 15.51209282875061\n",
            "timestamp: 1558015697\n",
            "timesteps_since_restore: 8000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 8000\n",
            "training_iteration: 2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT99085p9Kdn",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective (fewer SGD iterations and a larger batch size should help)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt0bHnS29Kdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "33603636-ee8d-4084-bc7d-698db5735595"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config['num_workers'] = 3\n",
        "config['num_sgd_iter'] = 10\n",
        "config['sgd_minibatch_size'] = 256\n",
        "config['model']['fcnet_hiddens'] = [20, 20]\n",
        "config['num_cpus_per_worker'] = 0\n",
        "\n",
        "agent = PPOAgent(config, 'CartPole-v0')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:09:38,350\tWARNING __init__.py:21 -- DeprecationWarning: PPOAgent has been renamed to PPOTrainer. This will raise an error in the future.\n",
            "2019-05-16 14:09:38,365\tWARNING ppo.py:171 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
            "2019-05-16 14:09:38,367\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2019-05-16 14:09:39,396\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7f32014c9ac8>}\n",
            "2019-05-16 14:09:39,398\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f32014c9588>}\n",
            "2019-05-16 14:09:39,399\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f32014c9898>}\n",
            "2019-05-16 14:09:39,468\tINFO multi_gpu_optimizer.py:78 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2019-05-16 14:09:39,447\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2019-05-16 14:09:39.461944: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2019-05-16 14:09:39.462244: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6e97340 executing computations on platform Host. Devices:\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2019-05-16 14:09:39.462279: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Colocations handled automatically by placer.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/action_dist.py:114: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Use tf.random.categorical instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:09:40,407\tERROR worker.py:1672 -- WARNING: 6 workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Use tf.cast instead.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=213)\u001b[0m Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdP8vI059Kdw",
        "colab_type": "text"
      },
      "source": [
        "**EXERCISE:** Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
        "\n",
        "This should take around 20 or 30 training iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHVqcoa9Kdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17457
        },
        "outputId": "5717c7be-3dad-42f2-ff5e-00824355ce7b"
      },
      "source": [
        "for i in range(20):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-37\n",
            "done: false\n",
            "episode_len_mean: 26.22222222222222\n",
            "episode_reward_max: 92.0\n",
            "episode_reward_mean: 26.22222222222222\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 153\n",
            "episodes_total: 504\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 580.712\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.05000000447034836\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6797142624855042\n",
            "      kl: 0.0025639538653194904\n",
            "      policy_loss: -0.007877949625253677\n",
            "      total_loss: 358.54803466796875\n",
            "      vf_explained_var: -5.960067210253328e-05\n",
            "      vf_loss: 358.5557861328125\n",
            "  load_time_ms: 22.756\n",
            "  num_steps_sampled: 12000\n",
            "  num_steps_trained: 11520\n",
            "  sample_time_ms: 2678.368\n",
            "  update_time_ms: 305.095\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09372710773863878\n",
            "  mean_inference_ms: 1.5282939755854676\n",
            "  mean_processing_ms: 0.2729578148158642\n",
            "time_since_restore: 10.876019716262817\n",
            "time_this_iter_s: 2.816330671310425\n",
            "time_total_s: 10.876019716262817\n",
            "timestamp: 1558015837\n",
            "timesteps_since_restore: 12000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 12000\n",
            "training_iteration: 3\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-40\n",
            "done: false\n",
            "episode_len_mean: 28.992805755395683\n",
            "episode_reward_max: 117.0\n",
            "episode_reward_mean: 28.992805755395683\n",
            "episode_reward_min: 9.0\n",
            "episodes_this_iter: 139\n",
            "episodes_total: 643\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 548.207\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.02500000223517418\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6646010279655457\n",
            "      kl: 0.0032619780395179987\n",
            "      policy_loss: -0.010435773991048336\n",
            "      total_loss: 437.1661071777344\n",
            "      vf_explained_var: 0.0005145867471583188\n",
            "      vf_loss: 437.17645263671875\n",
            "  load_time_ms: 17.328\n",
            "  num_steps_sampled: 16000\n",
            "  num_steps_trained: 15360\n",
            "  sample_time_ms: 2682.836\n",
            "  update_time_ms: 231.317\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09371815434902962\n",
            "  mean_inference_ms: 1.5399189663536992\n",
            "  mean_processing_ms: 0.2741477320182276\n",
            "time_since_restore: 14.052310943603516\n",
            "time_this_iter_s: 3.1762912273406982\n",
            "time_total_s: 14.052310943603516\n",
            "timestamp: 1558015840\n",
            "timesteps_since_restore: 16000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 16000\n",
            "training_iteration: 4\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-43\n",
            "done: false\n",
            "episode_len_mean: 34.29565217391304\n",
            "episode_reward_max: 101.0\n",
            "episode_reward_mean: 34.29565217391304\n",
            "episode_reward_min: 11.0\n",
            "episodes_this_iter: 115\n",
            "episodes_total: 758\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 525.485\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.01250000111758709\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6494041681289673\n",
            "      kl: 0.0031555104069411755\n",
            "      policy_loss: -0.012256717309355736\n",
            "      total_loss: 474.4007873535156\n",
            "      vf_explained_var: 0.0013173222541809082\n",
            "      vf_loss: 474.4129638671875\n",
            "  load_time_ms: 14.041\n",
            "  num_steps_sampled: 20000\n",
            "  num_steps_trained: 19200\n",
            "  sample_time_ms: 2675.081\n",
            "  update_time_ms: 187.092\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09396384342624252\n",
            "  mean_inference_ms: 1.5335758548407263\n",
            "  mean_processing_ms: 0.2714549189108558\n",
            "time_since_restore: 17.155842781066895\n",
            "time_this_iter_s: 3.103531837463379\n",
            "time_total_s: 17.155842781066895\n",
            "timestamp: 1558015843\n",
            "timesteps_since_restore: 20000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 20000\n",
            "training_iteration: 5\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-46\n",
            "done: false\n",
            "episode_len_mean: 41.22\n",
            "episode_reward_max: 111.0\n",
            "episode_reward_mean: 41.22\n",
            "episode_reward_min: 13.0\n",
            "episodes_this_iter: 96\n",
            "episodes_total: 854\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 511.044\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.006250000558793545\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6343461871147156\n",
            "      kl: 0.002104334067553282\n",
            "      policy_loss: -0.008950362913310528\n",
            "      total_loss: 620.5775756835938\n",
            "      vf_explained_var: 0.0017711917171254754\n",
            "      vf_loss: 620.5866088867188\n",
            "  load_time_ms: 11.902\n",
            "  num_steps_sampled: 24000\n",
            "  num_steps_trained: 23040\n",
            "  sample_time_ms: 2679.161\n",
            "  update_time_ms: 157.194\n",
            "iterations_since_restore: 6\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09364732554012482\n",
            "  mean_inference_ms: 1.5348815849710837\n",
            "  mean_processing_ms: 0.2676802784082665\n",
            "time_since_restore: 20.316357612609863\n",
            "time_this_iter_s: 3.1605148315429688\n",
            "time_total_s: 20.316357612609863\n",
            "timestamp: 1558015846\n",
            "timesteps_since_restore: 24000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 24000\n",
            "training_iteration: 6\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-49\n",
            "done: false\n",
            "episode_len_mean: 47.4\n",
            "episode_reward_max: 132.0\n",
            "episode_reward_mean: 47.4\n",
            "episode_reward_min: 13.0\n",
            "episodes_this_iter: 83\n",
            "episodes_total: 937\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 501.856\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0031250002793967724\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6306765675544739\n",
            "      kl: 0.0009191026911139488\n",
            "      policy_loss: -0.002326657297089696\n",
            "      total_loss: 763.565185546875\n",
            "      vf_explained_var: 0.0029080668464303017\n",
            "      vf_loss: 763.5674438476562\n",
            "  load_time_ms: 10.366\n",
            "  num_steps_sampled: 28000\n",
            "  num_steps_trained: 26880\n",
            "  sample_time_ms: 2683.344\n",
            "  update_time_ms: 135.647\n",
            "iterations_since_restore: 7\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09421303873332733\n",
            "  mean_inference_ms: 1.5400340754699002\n",
            "  mean_processing_ms: 0.26661835494810154\n",
            "time_since_restore: 23.49434542655945\n",
            "time_this_iter_s: 3.177987813949585\n",
            "time_total_s: 23.49434542655945\n",
            "timestamp: 1558015849\n",
            "timesteps_since_restore: 28000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 28000\n",
            "training_iteration: 7\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-53\n",
            "done: false\n",
            "episode_len_mean: 52.53\n",
            "episode_reward_max: 114.0\n",
            "episode_reward_mean: 52.53\n",
            "episode_reward_min: 11.0\n",
            "episodes_this_iter: 71\n",
            "episodes_total: 1008\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 503.998\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0015625001396983862\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6221932768821716\n",
            "      kl: 0.0005391281447373331\n",
            "      policy_loss: -0.004965054336935282\n",
            "      total_loss: 811.3226928710938\n",
            "      vf_explained_var: 0.002707835054025054\n",
            "      vf_loss: 811.3277587890625\n",
            "  load_time_ms: 9.217\n",
            "  num_steps_sampled: 32000\n",
            "  num_steps_trained: 30720\n",
            "  sample_time_ms: 2693.612\n",
            "  update_time_ms: 119.402\n",
            "iterations_since_restore: 8\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.0951967206981433\n",
            "  mean_inference_ms: 1.544249000902802\n",
            "  mean_processing_ms: 0.26571395772229367\n",
            "time_since_restore: 26.799621105194092\n",
            "time_this_iter_s: 3.3052756786346436\n",
            "time_total_s: 26.799621105194092\n",
            "timestamp: 1558015853\n",
            "timesteps_since_restore: 32000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 32000\n",
            "training_iteration: 8\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-56\n",
            "done: false\n",
            "episode_len_mean: 58.98\n",
            "episode_reward_max: 152.0\n",
            "episode_reward_mean: 58.98\n",
            "episode_reward_min: 11.0\n",
            "episodes_this_iter: 64\n",
            "episodes_total: 1072\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 497.278\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0007812500698491931\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6180635690689087\n",
            "      kl: 0.00041508686263114214\n",
            "      policy_loss: 0.00043884318438358605\n",
            "      total_loss: 958.8907470703125\n",
            "      vf_explained_var: 0.002923425054177642\n",
            "      vf_loss: 958.8902587890625\n",
            "  load_time_ms: 8.301\n",
            "  num_steps_sampled: 36000\n",
            "  num_steps_trained: 34560\n",
            "  sample_time_ms: 2694.72\n",
            "  update_time_ms: 107.088\n",
            "iterations_since_restore: 9\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09507009111248202\n",
            "  mean_inference_ms: 1.5510246591679036\n",
            "  mean_processing_ms: 0.2652282653330535\n",
            "time_since_restore: 29.968306064605713\n",
            "time_this_iter_s: 3.168684959411621\n",
            "time_total_s: 29.968306064605713\n",
            "timestamp: 1558015856\n",
            "timesteps_since_restore: 36000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 36000\n",
            "training_iteration: 9\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-10-59\n",
            "done: false\n",
            "episode_len_mean: 65.58\n",
            "episode_reward_max: 139.0\n",
            "episode_reward_mean: 65.58\n",
            "episode_reward_min: 14.0\n",
            "episodes_this_iter: 58\n",
            "episodes_total: 1130\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 493.817\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.00039062503492459655\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6107074022293091\n",
            "      kl: 0.0006757732480764389\n",
            "      policy_loss: -0.006896073464304209\n",
            "      total_loss: 994.8438720703125\n",
            "      vf_explained_var: 0.0072068930603563786\n",
            "      vf_loss: 994.850830078125\n",
            "  load_time_ms: 7.579\n",
            "  num_steps_sampled: 40000\n",
            "  num_steps_trained: 38400\n",
            "  sample_time_ms: 2695.432\n",
            "  update_time_ms: 97.09\n",
            "iterations_since_restore: 10\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09515771441561967\n",
            "  mean_inference_ms: 1.5573510387789404\n",
            "  mean_processing_ms: 0.26510767184255263\n",
            "time_since_restore: 33.155585289001465\n",
            "time_this_iter_s: 3.187279224395752\n",
            "time_total_s: 33.155585289001465\n",
            "timestamp: 1558015859\n",
            "timesteps_since_restore: 40000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 40000\n",
            "training_iteration: 10\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-02\n",
            "done: false\n",
            "episode_len_mean: 67.24\n",
            "episode_reward_max: 153.0\n",
            "episode_reward_mean: 67.24\n",
            "episode_reward_min: 14.0\n",
            "episodes_this_iter: 63\n",
            "episodes_total: 1193\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 456.117\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.00019531251746229827\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.608718991279602\n",
            "      kl: 0.0003722461115103215\n",
            "      policy_loss: -0.001058959518559277\n",
            "      total_loss: 882.6856079101562\n",
            "      vf_explained_var: 0.006081811618059874\n",
            "      vf_loss: 882.6866455078125\n",
            "  load_time_ms: 1.101\n",
            "  num_steps_sampled: 44000\n",
            "  num_steps_trained: 42240\n",
            "  sample_time_ms: 2682.565\n",
            "  update_time_ms: 8.6\n",
            "iterations_since_restore: 11\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09624486089926984\n",
            "  mean_inference_ms: 1.5633818864958322\n",
            "  mean_processing_ms: 0.2648106966271303\n",
            "time_since_restore: 36.45563077926636\n",
            "time_this_iter_s: 3.3000454902648926\n",
            "time_total_s: 36.45563077926636\n",
            "timestamp: 1558015862\n",
            "timesteps_since_restore: 44000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 44000\n",
            "training_iteration: 11\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-05\n",
            "done: false\n",
            "episode_len_mean: 69.18\n",
            "episode_reward_max: 181.0\n",
            "episode_reward_mean: 69.18\n",
            "episode_reward_min: 18.0\n",
            "episodes_this_iter: 55\n",
            "episodes_total: 1248\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 461.026\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 9.765625873114914e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5905288457870483\n",
            "      kl: 0.001126591581851244\n",
            "      policy_loss: -0.0033502872101962566\n",
            "      total_loss: 1118.5184326171875\n",
            "      vf_explained_var: 0.009282310493290424\n",
            "      vf_loss: 1118.521728515625\n",
            "  load_time_ms: 1.101\n",
            "  num_steps_sampled: 48000\n",
            "  num_steps_trained: 46080\n",
            "  sample_time_ms: 2678.539\n",
            "  update_time_ms: 8.158\n",
            "iterations_since_restore: 12\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09651219108308016\n",
            "  mean_inference_ms: 1.5656661110211667\n",
            "  mean_processing_ms: 0.26355892047842366\n",
            "time_since_restore: 39.69162607192993\n",
            "time_this_iter_s: 3.235995292663574\n",
            "time_total_s: 39.69162607192993\n",
            "timestamp: 1558015865\n",
            "timesteps_since_restore: 48000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 48000\n",
            "training_iteration: 12\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-09\n",
            "done: false\n",
            "episode_len_mean: 75.18\n",
            "episode_reward_max: 192.0\n",
            "episode_reward_mean: 75.18\n",
            "episode_reward_min: 18.0\n",
            "episodes_this_iter: 50\n",
            "episodes_total: 1298\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 461.57\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 4.882812936557457e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5867528915405273\n",
            "      kl: 0.0007520588696934283\n",
            "      policy_loss: -0.0004994458286091685\n",
            "      total_loss: 1122.7508544921875\n",
            "      vf_explained_var: 0.011586224660277367\n",
            "      vf_loss: 1122.75146484375\n",
            "  load_time_ms: 1.062\n",
            "  num_steps_sampled: 52000\n",
            "  num_steps_trained: 49920\n",
            "  sample_time_ms: 2718.629\n",
            "  update_time_ms: 9.429\n",
            "iterations_since_restore: 13\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09704104633163381\n",
            "  mean_inference_ms: 1.5668277849765784\n",
            "  mean_processing_ms: 0.26203399951022255\n",
            "time_since_restore: 42.924041509628296\n",
            "time_this_iter_s: 3.2324154376983643\n",
            "time_total_s: 42.924041509628296\n",
            "timestamp: 1558015869\n",
            "timesteps_since_restore: 52000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 52000\n",
            "training_iteration: 13\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-12\n",
            "done: false\n",
            "episode_len_mean: 77.45\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 77.45\n",
            "episode_reward_min: 20.0\n",
            "episodes_this_iter: 53\n",
            "episodes_total: 1351\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 461.996\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 2.4414064682787284e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5784714818000793\n",
            "      kl: 0.0013669788604602218\n",
            "      policy_loss: -0.007796341087669134\n",
            "      total_loss: 1105.38427734375\n",
            "      vf_explained_var: 0.007251055911183357\n",
            "      vf_loss: 1105.3919677734375\n",
            "  load_time_ms: 1.075\n",
            "  num_steps_sampled: 56000\n",
            "  num_steps_trained: 53760\n",
            "  sample_time_ms: 2716.317\n",
            "  update_time_ms: 9.436\n",
            "iterations_since_restore: 14\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.0971607676364263\n",
            "  mean_inference_ms: 1.5681582298737533\n",
            "  mean_processing_ms: 0.26121831330851386\n",
            "time_since_restore: 46.075780391693115\n",
            "time_this_iter_s: 3.1517388820648193\n",
            "time_total_s: 46.075780391693115\n",
            "timestamp: 1558015872\n",
            "timesteps_since_restore: 56000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 56000\n",
            "training_iteration: 14\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-15\n",
            "done: false\n",
            "episode_len_mean: 84.55\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 84.55\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 38\n",
            "episodes_total: 1389\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 465.714\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 1.2207032341393642e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5808202028274536\n",
            "      kl: 0.001843297271989286\n",
            "      policy_loss: -0.0017603258602321148\n",
            "      total_loss: 1279.327880859375\n",
            "      vf_explained_var: 0.015269231982529163\n",
            "      vf_loss: 1279.3297119140625\n",
            "  load_time_ms: 1.102\n",
            "  num_steps_sampled: 60000\n",
            "  num_steps_trained: 57600\n",
            "  sample_time_ms: 2720.847\n",
            "  update_time_ms: 9.064\n",
            "iterations_since_restore: 15\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09709029323682007\n",
            "  mean_inference_ms: 1.5680466130122463\n",
            "  mean_processing_ms: 0.2612728919669285\n",
            "time_since_restore: 49.256671667099\n",
            "time_this_iter_s: 3.180891275405884\n",
            "time_total_s: 49.256671667099\n",
            "timestamp: 1558015875\n",
            "timesteps_since_restore: 60000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 60000\n",
            "training_iteration: 15\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-18\n",
            "done: false\n",
            "episode_len_mean: 99.67\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 99.67\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 36\n",
            "episodes_total: 1425\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 475.71\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 6.103516170696821e-06\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5855404138565063\n",
            "      kl: 0.002660329919308424\n",
            "      policy_loss: -0.0052077146247029305\n",
            "      total_loss: 1700.7835693359375\n",
            "      vf_explained_var: 0.014988688752055168\n",
            "      vf_loss: 1700.788818359375\n",
            "  load_time_ms: 1.111\n",
            "  num_steps_sampled: 64000\n",
            "  num_steps_trained: 61440\n",
            "  sample_time_ms: 2725.479\n",
            "  update_time_ms: 9.477\n",
            "iterations_since_restore: 16\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09703148558590771\n",
            "  mean_inference_ms: 1.5702791503460003\n",
            "  mean_processing_ms: 0.2618473769667471\n",
            "time_since_restore: 52.568456172943115\n",
            "time_this_iter_s: 3.311784505844116\n",
            "time_total_s: 52.568456172943115\n",
            "timestamp: 1558015878\n",
            "timesteps_since_restore: 64000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 64000\n",
            "training_iteration: 16\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-22\n",
            "done: false\n",
            "episode_len_mean: 112.69\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 112.69\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 31\n",
            "episodes_total: 1456\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 481.993\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 3.0517580853484105e-06\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5794509649276733\n",
            "      kl: 0.001269470201805234\n",
            "      policy_loss: -0.008654935285449028\n",
            "      total_loss: 1620.6402587890625\n",
            "      vf_explained_var: 0.017008798196911812\n",
            "      vf_loss: 1620.64892578125\n",
            "  load_time_ms: 1.09\n",
            "  num_steps_sampled: 68000\n",
            "  num_steps_trained: 65280\n",
            "  sample_time_ms: 2732.197\n",
            "  update_time_ms: 9.47\n",
            "iterations_since_restore: 17\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09716956775611596\n",
            "  mean_inference_ms: 1.568635718730867\n",
            "  mean_processing_ms: 0.2613742239268173\n",
            "time_since_restore: 55.87537980079651\n",
            "time_this_iter_s: 3.3069236278533936\n",
            "time_total_s: 55.87537980079651\n",
            "timestamp: 1558015882\n",
            "timesteps_since_restore: 68000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 68000\n",
            "training_iteration: 17\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-25\n",
            "done: false\n",
            "episode_len_mean: 121.02\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 121.02\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 32\n",
            "episodes_total: 1488\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 484.905\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 1.5258790426742053e-06\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5906704664230347\n",
            "      kl: 0.0006440415745601058\n",
            "      policy_loss: -0.002706420375034213\n",
            "      total_loss: 1732.85205078125\n",
            "      vf_explained_var: 0.01983940228819847\n",
            "      vf_loss: 1732.8543701171875\n",
            "  load_time_ms: 1.08\n",
            "  num_steps_sampled: 72000\n",
            "  num_steps_trained: 69120\n",
            "  sample_time_ms: 2744.663\n",
            "  update_time_ms: 9.394\n",
            "iterations_since_restore: 18\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.0975530029681285\n",
            "  mean_inference_ms: 1.5719684246414494\n",
            "  mean_processing_ms: 0.26138678033797574\n",
            "time_since_restore: 59.33358979225159\n",
            "time_this_iter_s: 3.458209991455078\n",
            "time_total_s: 59.33358979225159\n",
            "timestamp: 1558015885\n",
            "timesteps_since_restore: 72000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 72000\n",
            "training_iteration: 18\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-28\n",
            "done: false\n",
            "episode_len_mean: 131.18\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 131.18\n",
            "episode_reward_min: 41.0\n",
            "episodes_this_iter: 27\n",
            "episodes_total: 1515\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 487.354\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 7.629395213371026e-07\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5870649218559265\n",
            "      kl: 5.2604689699364826e-05\n",
            "      policy_loss: -0.0040775020606815815\n",
            "      total_loss: 1735.6484375\n",
            "      vf_explained_var: 0.01891821250319481\n",
            "      vf_loss: 1735.652587890625\n",
            "  load_time_ms: 1.075\n",
            "  num_steps_sampled: 76000\n",
            "  num_steps_trained: 72960\n",
            "  sample_time_ms: 2746.452\n",
            "  update_time_ms: 9.935\n",
            "iterations_since_restore: 19\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09799095189582215\n",
            "  mean_inference_ms: 1.5782476498913411\n",
            "  mean_processing_ms: 0.261824110058604\n",
            "time_since_restore: 62.551400661468506\n",
            "time_this_iter_s: 3.217810869216919\n",
            "time_total_s: 62.551400661468506\n",
            "timestamp: 1558015888\n",
            "timesteps_since_restore: 76000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 76000\n",
            "training_iteration: 19\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-32\n",
            "done: false\n",
            "episode_len_mean: 137.63\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 137.63\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 27\n",
            "episodes_total: 1542\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 487.078\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 3.814697606685513e-07\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.585568368434906\n",
            "      kl: 0.0011173866223543882\n",
            "      policy_loss: 0.003018787829205394\n",
            "      total_loss: 1887.500244140625\n",
            "      vf_explained_var: 0.012349938973784447\n",
            "      vf_loss: 1887.4974365234375\n",
            "  load_time_ms: 1.061\n",
            "  num_steps_sampled: 80000\n",
            "  num_steps_trained: 76800\n",
            "  sample_time_ms: 2745.042\n",
            "  update_time_ms: 10.207\n",
            "iterations_since_restore: 20\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09822716090378099\n",
            "  mean_inference_ms: 1.5816764181335248\n",
            "  mean_processing_ms: 0.2618774388615712\n",
            "time_since_restore: 65.7219443321228\n",
            "time_this_iter_s: 3.170543670654297\n",
            "time_total_s: 65.7219443321228\n",
            "timestamp: 1558015892\n",
            "timesteps_since_restore: 80000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 80000\n",
            "training_iteration: 20\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-35\n",
            "done: false\n",
            "episode_len_mean: 145.0\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 145.0\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 27\n",
            "episodes_total: 1569\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 485.852\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 1.9073488033427566e-07\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5815298557281494\n",
            "      kl: 0.001676378189586103\n",
            "      policy_loss: -0.003532510483637452\n",
            "      total_loss: 1794.39990234375\n",
            "      vf_explained_var: 0.016091363504529\n",
            "      vf_loss: 1794.403564453125\n",
            "  load_time_ms: 1.085\n",
            "  num_steps_sampled: 84000\n",
            "  num_steps_trained: 80640\n",
            "  sample_time_ms: 2731.001\n",
            "  update_time_ms: 9.936\n",
            "iterations_since_restore: 21\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09825962647983807\n",
            "  mean_inference_ms: 1.582556652892454\n",
            "  mean_processing_ms: 0.26133973224222\n",
            "time_since_restore: 68.86676359176636\n",
            "time_this_iter_s: 3.1448192596435547\n",
            "time_total_s: 68.86676359176636\n",
            "timestamp: 1558015895\n",
            "timesteps_since_restore: 84000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 84000\n",
            "training_iteration: 21\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-16_14-11-38\n",
            "done: false\n",
            "episode_len_mean: 147.38\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 147.38\n",
            "episode_reward_min: 25.0\n",
            "episodes_this_iter: 28\n",
            "episodes_total: 1597\n",
            "experiment_id: 7c4787f1bf084d65857dc9dca012a585\n",
            "hostname: f5120e65a079\n",
            "info:\n",
            "  grad_time_ms: 484.602\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 9.536744016713783e-08\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5716301798820496\n",
            "      kl: 0.0009586411761119962\n",
            "      policy_loss: -0.0025183786638081074\n",
            "      total_loss: 1623.473388671875\n",
            "      vf_explained_var: 0.013640216551721096\n",
            "      vf_loss: 1623.47607421875\n",
            "  load_time_ms: 1.114\n",
            "  num_steps_sampled: 88000\n",
            "  num_steps_trained: 84480\n",
            "  sample_time_ms: 2730.489\n",
            "  update_time_ms: 10.474\n",
            "iterations_since_restore: 22\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 118\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.09822530769576027\n",
            "  mean_inference_ms: 1.5820885436888603\n",
            "  mean_processing_ms: 0.2607096048751707\n",
            "time_since_restore: 72.08964443206787\n",
            "time_this_iter_s: 3.2228808403015137\n",
            "time_total_s: 72.08964443206787\n",
            "timestamp: 1558015898\n",
            "timesteps_since_restore: 88000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 88000\n",
            "training_iteration: 22\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhGwDPfZ9Kd8",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint the current model. The call to `agent.save()` returns the path to the checkpointed model and can be used later to restore the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZCVpHgM9KeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7cfb9fd-15e2-4320-afad-aeb70838e6b7"
      },
      "source": [
        "checkpoint_path = agent.save()\n",
        "print(checkpoint_path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/ray_results/PPO_CartPole-v0_2019-05-16_14-09-38m1nxh4tk/checkpoint_22/checkpoint-22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaIp3AJB9KeG",
        "colab_type": "text"
      },
      "source": [
        "Now let's use the trained policy to make predictions.\n",
        "\n",
        "**NOTE:** Here we are loading the trained policy in the same process, but in practice, this would often be done in a different process (probably on a different machine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etrpsyyo9KeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "outputId": "cf2575e6-bddc-43e3-f68c-398c7a1099bf"
      },
      "source": [
        "trained_config = config.copy()\n",
        "\n",
        "test_agent = PPOAgent(trained_config, 'CartPole-v0')\n",
        "test_agent.restore(checkpoint_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-16 14:12:39,709\tWARNING __init__.py:21 -- DeprecationWarning: PPOAgent has been renamed to PPOTrainer. This will raise an error in the future.\n",
            "2019-05-16 14:12:39,725\tWARNING ppo.py:171 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
            "2019-05-16 14:12:39,729\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2019-05-16 14:12:41,167\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7f31fd57f4a8>}\n",
            "2019-05-16 14:12:41,169\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f31fd57fb00>}\n",
            "2019-05-16 14:12:41,176\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f31fd57fe10>}\n",
            "2019-05-16 14:12:41,251\tINFO multi_gpu_optimizer.py:78 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m 2019-05-16 14:12:41,231\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m 2019-05-16 14:12:41,218\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m 2019-05-16 14:12:41.226316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m 2019-05-16 14:12:41.226559: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6d7d340 executing computations on platform Host. Devices:\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m 2019-05-16 14:12:41.226599: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m 2019-05-16 14:12:41.248412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m 2019-05-16 14:12:41.248659: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x73b1340 executing computations on platform Host. Devices:\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m 2019-05-16 14:12:41.248684: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Colocations handled automatically by placer.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Colocations handled automatically by placer.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/action_dist.py:114: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Use tf.random.categorical instead.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/action_dist.py:114: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Use tf.random.categorical instead.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Use tf.cast instead.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Use tf.cast instead.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=470)\u001b[0m Deprecated in favor of operator or tf.math.divide.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=468)\u001b[0m Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91647Faa9KeY",
        "colab_type": "text"
      },
      "source": [
        "Now use the trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action.\n",
        "\n",
        "**EXERCISE:** Verify that the reward received roughly matches up with the reward printed in the training logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuF76b4C9KeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02466940-650d-4007-d48a-23d245508d80"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = test_agent.compute_action(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    cumulative_reward += reward\n",
        "\n",
        "print(cumulative_reward)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeO_-3Os9Kef",
        "colab_type": "text"
      },
      "source": [
        "## Visualize results with TensorBoard\n",
        "\n",
        "**EXERCISE**: Finally, you can visualize your training results using TensorBoard. To do this, open a new terminal in Jupyter lab using the \"+\" button, and run:\n",
        "    \n",
        "`$ tensorboard --logdir=~/ray_results --host=0.0.0.0`\n",
        "\n",
        "And open your browser to the address printed (or change the current URL to go to port 6006). Check the \"episode_reward_mean\" learning curve of the PPO agent. Toggle the horizontal axis between both the \"STEPS\" and \"RELATIVE\" view to compare efficiency in number of timesteps vs real time time.\n",
        "\n",
        "Note that TensorBoard will not work in Binder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnsSlSOx_jVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}